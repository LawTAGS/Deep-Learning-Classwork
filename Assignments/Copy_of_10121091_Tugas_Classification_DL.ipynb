{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVwWkShDekucKHs9OtxvrD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LawTAGS/Deep-Learning-Classwork/blob/main/Assignments/Copy_of_10121091_Tugas_Classification_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Binary Classification Using NumPy**"
      ],
      "metadata": {
        "id": "gOC_xPjqeheD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Functions\n",
        "def softmax(z):\n",
        "  exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "  return exp_z/np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Binary Cross-Entropy Loss\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "  m = y_true.shape[0]\n",
        "  loss = -np.mean(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))\n",
        "  return loss\n",
        "\n",
        "# Multi-Layer Perceptron (MLP)\n",
        "class BinaryMLP:\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
        "    self.b1 = np.zeros((1, hidden_size))\n",
        "    self.W2 = np.random.randn(hidden_size, 2) * np.sqrt(2 / hidden_size)\n",
        "    self.b2 = np.zeros((1, 2))\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.z1 = np.dot(X, self.W1) + self.b1\n",
        "    self.a1 = np.maximum(0, self.z1)\n",
        "    self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "    self.a2 = softmax(self.z2)\n",
        "    return self.a2\n",
        "\n",
        "  def backward(self, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Correct Softmax derivative for BCE: dL/dz = y_pred - y\n",
        "    dz2 = self.a2 - y # Output Layer gradient (Softmax + BCE)\n",
        "    dW2 = np.dot(self.a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden Layer error\n",
        "    dz1 = np.dot(dz2, self.W2.T) * (self.z1 > 0)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Update weights and biases\n",
        "    self.W1 -= learning_rate * dW1\n",
        "    self.b1 -= learning_rate * db1\n",
        "    self.W2 -= learning_rate * dW2\n",
        "    self.b2 -= learning_rate * db2\n",
        "\n",
        "  def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
        "    for epoch in range(epochs):\n",
        "      y_pred = self.forward(X)\n",
        "      loss = binary_cross_entropy(y, y_pred)\n",
        "      self.backward(X, y, learning_rate)\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "  def predict(self, X):\n",
        "    y_pred = self.forward(X)\n",
        "    return np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Generate Synthetic Binary Data\n",
        "np.random.seed(10121091)\n",
        "\n",
        "# Generate 2D features (simple binary classification)\n",
        "X_data = np.random.randn(1000, 2)\n",
        "y_data = (X_data[:, 0] + X_data[:, 1] > 0).astype(int)\n",
        "\n",
        "# Convert labels to one-hot encoding for Softmax\n",
        "y_data_one_hot = np.zeros((y_data.shape[0], 2))\n",
        "y_data_one_hot[np.arange(y_data.size), y_data] = 1\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "split_ratio = 0.7\n",
        "split_index = int(len(X_data) * split_ratio)\n",
        "\n",
        "X_train, X_test = X_data[:split_index], X_data[split_index:]\n",
        "y_train, y_test = y_data_one_hot[:split_index], y_data_one_hot[split_index:]\n",
        "\n",
        "# Train the MLP\n",
        "mlp = BinaryMLP(input_size=2, hidden_size=10)\n",
        "mlp.train(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = mlp.predict(X_test)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhn_dDZ1TY8d",
        "outputId": "3d0547ba-e4dd-4179-95da-821051d12fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.2962\n",
            "Epoch 100, Loss: 0.4156\n",
            "Epoch 200, Loss: 0.2773\n",
            "Epoch 300, Loss: 0.2187\n",
            "Epoch 400, Loss: 0.1828\n",
            "Epoch 500, Loss: 0.1577\n",
            "Epoch 600, Loss: 0.1392\n",
            "Epoch 700, Loss: 0.1251\n",
            "Epoch 800, Loss: 0.1139\n",
            "Epoch 900, Loss: 0.1050\n",
            "Test Accuracy: 0.9967%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Multi Layers Binary Classification Using NumPy**"
      ],
      "metadata": {
        "id": "_ZkGCS5lfDFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Functions\n",
        "def softmax(z):\n",
        "    \"\"\"Softmax activation function.\"\"\"\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Prevents overflow\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Binary Cross-Entropy Loss\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"Computes Binary Cross-Entropy loss.\"\"\"\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.mean(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))  # Avoid log(0)\n",
        "    return loss\n",
        "\n",
        "# Multi-Layer Perceptron (MLP)\n",
        "class CustomBinaryMLP:\n",
        "    def __init__(self, layer_sizes):\n",
        "        \"\"\"\n",
        "        Initializes a multi-layer MLP for binary classification.\n",
        "\n",
        "        layer_sizes: List containing number of neurons in each layer, including input and output.\n",
        "        Example: [2, 10, 5, 2] --> 2 inputs → 10 hidden → 5 hidden → 2 output (Softmax)\n",
        "        \"\"\"\n",
        "        self.num_layers = len(layer_sizes) - 1\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        for i in range(self.num_layers):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i]))  # He Init\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation through multiple layers.\n",
        "        \"\"\"\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(self.num_layers - 1):  # Hidden layers\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            a = np.maximum(0, z)  # ReLU Activation\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(a)\n",
        "\n",
        "        # Output layer (Softmax)\n",
        "        z_out = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        a_out = softmax(z_out)\n",
        "        self.z_values.append(z_out)\n",
        "        self.activations.append(a_out)\n",
        "\n",
        "        return a_out\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Backpropagation for multi-layer MLP.\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        dz = self.activations[-1] - y  # Output layer gradient (Softmax + BCE)\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        for i in range(self.num_layers - 1, 0, -1):  # Hidden layers\n",
        "            dW = np.dot(self.activations[i].T, dz) / m\n",
        "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "            dz = np.dot(dz, self.weights[i].T) * (self.z_values[i - 1] > 0)  # ReLU derivative\n",
        "\n",
        "            # Update weights\n",
        "            self.weights[i] -= learning_rate * dW\n",
        "            self.biases[i] -= learning_rate * db\n",
        "\n",
        "        # First layer update\n",
        "        dW = np.dot(X.T, dz) / m\n",
        "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "        self.weights[0] -= learning_rate * dW\n",
        "        self.biases[0] -= learning_rate * db\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
        "        \"\"\"\n",
        "        Train the MLP model.\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = binary_cross_entropy(y, y_pred)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels (0 or 1).\n",
        "        \"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1)  # Return the class with the highest probability\n",
        "\n",
        "# Generate Synthetic Binary Data\n",
        "np.random.seed(10121091)\n",
        "\n",
        "# Generate 2D features (simple binary classification)\n",
        "X_data = np.random.randn(1000, 2)\n",
        "y_data = (X_data[:, 0] + X_data[:, 1] > 0).astype(int)  # Class 1 if sum > 0, else Class 0\n",
        "\n",
        "# Convert labels to one-hot encoding for Softmax\n",
        "y_data_one_hot = np.zeros((y_data.size, 2))\n",
        "y_data_one_hot[np.arange(y_data.size), y_data] = 1\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "split_ratio = 0.7\n",
        "split_index = int(len(X_data) * split_ratio)\n",
        "\n",
        "X_train, X_test = X_data[:split_index], X_data[split_index:]\n",
        "y_train, y_test = y_data_one_hot[:split_index], y_data_one_hot[split_index:]\n",
        "\n",
        "# Train the Custom MLP\n",
        "\n",
        "# Define architecture: [input_size, hidden_layer1, hidden_layer2, ..., output_size]\n",
        "layer_sizes = [2, 20, 10, 5, 2]  # Custom hidden layers\n",
        "\n",
        "mlp = CustomBinaryMLP(layer_sizes)\n",
        "mlp.train(X_train, y_train, learning_rate=0.01, epochs=2000)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = mlp.predict(X_test)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pb9-oYfYzYc",
        "outputId": "ca9d87d4-1ace-478a-e27a-ebda76310bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.5042\n",
            "Epoch 100, Loss: 0.2318\n",
            "Epoch 200, Loss: 0.1410\n",
            "Epoch 300, Loss: 0.1035\n",
            "Epoch 400, Loss: 0.0824\n",
            "Epoch 500, Loss: 0.0687\n",
            "Epoch 600, Loss: 0.0591\n",
            "Epoch 700, Loss: 0.0521\n",
            "Epoch 800, Loss: 0.0467\n",
            "Epoch 900, Loss: 0.0425\n",
            "Epoch 1000, Loss: 0.0391\n",
            "Epoch 1100, Loss: 0.0363\n",
            "Epoch 1200, Loss: 0.0338\n",
            "Epoch 1300, Loss: 0.0318\n",
            "Epoch 1400, Loss: 0.0300\n",
            "Epoch 1500, Loss: 0.0284\n",
            "Epoch 1600, Loss: 0.0270\n",
            "Epoch 1700, Loss: 0.0258\n",
            "Epoch 1800, Loss: 0.0247\n",
            "Epoch 1900, Loss: 0.0237\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Multiclassification Using NumPy**"
      ],
      "metadata": {
        "id": "BPby3ZbmfT-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input Parameters\n",
        "N = 1000        # Number of data samples\n",
        "num_classes = 3 # Number of target classes\n",
        "\n",
        "# Activation Functions\n",
        "def softmax(z):\n",
        "    \"\"\"Softmax activation function applied row-wise.\"\"\"\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Categorical Cross-Entropy Loss\n",
        "def categorical_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"Computes Categorical Cross-Entropy loss.\n",
        "\n",
        "    y_true is one-hot encoded.\n",
        "    \"\"\"\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.mean(np.log(y_pred[np.arange(m), np.argmax(y_true, axis=1)] + 1e-9))\n",
        "    return loss\n",
        "\n",
        "def generate_labels(X_data, num_classes):\n",
        "    \"\"\"\n",
        "    Generate classification labels for any number of classes using a general rule.\n",
        "    \"\"\"\n",
        "    N = X_data.shape[0]\n",
        "    y_data = np.zeros(N, dtype=int)  # Default all to class 0\n",
        "\n",
        "    if num_classes == 1:\n",
        "        return y_data  # All belong to class 0\n",
        "\n",
        "    elif num_classes == 2:\n",
        "        return (X_data[:, 0] > 0).astype(int)  # Right (1) vs Left (0)\n",
        "\n",
        "    elif num_classes == 3:\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0  # First quadrant\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1  # Second quadrant\n",
        "        y_data[X_data[:, 1] <= 0] = 2  # Bottom half\n",
        "\n",
        "    elif num_classes == 4:\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0  # First quadrant\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1  # Second quadrant\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] <= 0)] = 2  # Third quadrant\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] <= 0)] = 3  # Fourth quadrant\n",
        "\n",
        "    elif num_classes == 5:\n",
        "        center_mask = (np.abs(X_data[:, 0]) < 0.5) & (np.abs(X_data[:, 1]) < 0.5)\n",
        "        y_data[center_mask] = 4  # Center region\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0  # First quadrant\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1  # Second quadrant\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] <= 0)] = 2  # Third quadrant\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] <= 0)] = 3  # Fourth quadrant\n",
        "\n",
        "    else:\n",
        "        # General rule for any number of classes > 5\n",
        "        angles = np.arctan2(X_data[:, 1], X_data[:, 0])\n",
        "        angles[angles < 0] += 2 * np.pi\n",
        "        sector_size = 2 * np.pi / num_classes\n",
        "        y_data = (angles // sector_size).astype(int)\n",
        "    return y_data\n",
        "\n",
        "# Multi-Class MLP with Custom Hidden Layers\n",
        "class MultiClassMLP:\n",
        "    def __init__(self, layer_sizes):\n",
        "        \"\"\"\n",
        "        Initializes a multi-layer perceptron for multi-class classification.\n",
        "        \"\"\"\n",
        "        self.num_layers = len(layer_sizes) - 1\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation through the network.\"\"\"\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            a = np.maximum(0, z)  # ReLU activation\n",
        "            self.activations.append(a)\n",
        "\n",
        "        z_out = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z_out)\n",
        "        a_out = softmax(z_out)\n",
        "        self.activations.append(a_out)\n",
        "        return a_out\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"Backpropagation to compute gradients and update weights.\"\"\"\n",
        "        m = X.shape[0]\n",
        "        delta = self.activations[-1] - y\n",
        "\n",
        "        for i in range(self.num_layers - 1, 0, -1):\n",
        "            dW = np.dot(self.activations[i].T, delta) / m\n",
        "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
        "            self.weights[i] -= learning_rate * dW\n",
        "            self.biases[i]  -= learning_rate * db\n",
        "            delta = np.dot(delta, self.weights[i].T)\n",
        "            delta = delta * (self.z_values[i-1] > 0)\n",
        "\n",
        "        dW = np.dot(X.T, delta) / m\n",
        "        db = np.sum(delta, axis=0, keepdims=True) / m\n",
        "        self.weights[0] -= learning_rate * dW\n",
        "        self.biases[0]  -= learning_rate * db\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
        "        \"\"\"Train the MLP model.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = categorical_cross_entropy(y, y_pred)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels.\"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Generate Synthetic Multi-Class Data\n",
        "np.random.seed(10121091)\n",
        "X_data = np.random.randn(N, 2)\n",
        "\n",
        "y_data = generate_labels(X_data, num_classes)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_data_one_hot = np.zeros((N, num_classes))\n",
        "y_data_one_hot[np.arange(N), y_data] = 1\n",
        "\n",
        "# Split Data into Training and Testing Sets\n",
        "split_ratio = 0.7\n",
        "split_index = int(N * split_ratio)\n",
        "X_train, X_test = X_data[:split_index], X_data[split_index:]\n",
        "y_train, y_test = y_data_one_hot[:split_index], y_data_one_hot[split_index:]\n",
        "\n",
        "# Train the MLP\n",
        "layer_sizes = [2, 20, 10, num_classes]\n",
        "mlp = MultiClassMLP(layer_sizes)\n",
        "mlp.train(X_train, y_train, learning_rate=0.01, epochs=1000)\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-FgMbiWZq4G",
        "outputId": "b4d3973f-125c-4c4c-8066-c4f45d42e991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.5228\n",
            "Epoch 100, Loss: 0.6609\n",
            "Epoch 200, Loss: 0.4051\n",
            "Epoch 300, Loss: 0.2831\n",
            "Epoch 400, Loss: 0.2183\n",
            "Epoch 500, Loss: 0.1800\n",
            "Epoch 600, Loss: 0.1547\n",
            "Epoch 700, Loss: 0.1366\n",
            "Epoch 800, Loss: 0.1229\n",
            "Epoch 900, Loss: 0.1122\n",
            "Test Accuracy: 0.9867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Multiclassification Using PyTorch**"
      ],
      "metadata": {
        "id": "FzwXuxMXfbKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Input Parameters\n",
        "N = 1000           # Number of data samples\n",
        "num_classes = 8    # Number of target classes\n",
        "\n",
        "# Data Generation\n",
        "np.random.seed(10121091)\n",
        "X_data = np.random.randn(N, 2)\n",
        "\n",
        "def generate_labels(X_data, num_classes):\n",
        "    \"\"\"\n",
        "    Generate classification labels for any number of classes using a general rule.\n",
        "    \"\"\"\n",
        "    N = X_data.shape[0]\n",
        "    y_data = np.zeros(N, dtype=int)  # Default all to class 0\n",
        "\n",
        "    if num_classes == 1:\n",
        "        return y_data\n",
        "\n",
        "    elif num_classes == 2:\n",
        "        return (X_data[:, 0] > 0).astype(int)\n",
        "\n",
        "    elif num_classes == 3:\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1\n",
        "        y_data[X_data[:, 1] <= 0] = 2\n",
        "\n",
        "    elif num_classes == 4:\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] <= 0)] = 2\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] <= 0)] = 3\n",
        "\n",
        "    elif num_classes == 5:\n",
        "        center_mask = (np.abs(X_data[:, 0]) < 0.5) & (np.abs(X_data[:, 1]) < 0.5)\n",
        "        y_data[center_mask] = 4\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] <= 0)] = 2\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] <= 0)] = 3\n",
        "\n",
        "    else:\n",
        "        angles = np.arctan2(X_data[:, 1], X_data[:, 0])\n",
        "        angles[angles < 0] += 2 * np.pi\n",
        "\n",
        "        sector_size = 2 * np.pi / num_classes\n",
        "        y_data = (angles // sector_size).astype(int)\n",
        "    return y_data\n",
        "\n",
        "y_data = generate_labels(X_data, num_classes)\n",
        "\n",
        "# Split data (70% train, 30% test)\n",
        "split_index = int(0.7 * N)\n",
        "X_train = X_data[:split_index]\n",
        "X_test  = X_data[split_index:]\n",
        "y_train = y_data[:split_index]\n",
        "y_test  = y_data[split_index:]\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define the MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        \"\"\"\n",
        "        layer_sizes: List of layer sizes, e.g., [2, 20, 10, num_classes]\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
        "            if i < len(layer_sizes) - 2:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "layer_sizes = [2, 20, 10, num_classes]\n",
        "model = MLP(layer_sizes)\n",
        "\n",
        "# Apply He initialization\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    outputs_test = model(X_test_tensor)\n",
        "    predicted = torch.argmax(outputs_test, dim=1)\n",
        "    accuracy = (predicted == y_test_tensor).float().mean().item()\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbI_elI-Zw-d",
        "outputId": "0c3938ff-e418-4fff-b9b1-a177ec8baf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.5472\n",
            "Epoch 100, Loss: 0.1419\n",
            "Epoch 200, Loss: 0.0571\n",
            "Epoch 300, Loss: 0.0324\n",
            "Epoch 400, Loss: 0.0185\n",
            "Epoch 500, Loss: 0.0116\n",
            "Epoch 600, Loss: 0.0085\n",
            "Epoch 700, Loss: 0.0067\n",
            "Epoch 800, Loss: 0.0056\n",
            "Epoch 900, Loss: 0.0047\n",
            "Test Accuracy: 0.9767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Multiclassification Using SKLearn**"
      ],
      "metadata": {
        "id": "fx26g-o8fmR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Input Parameters\n",
        "N = 1000           # Number of data samples\n",
        "num_classes = 5    # Number of target classes\n",
        "\n",
        "# Data Generation\n",
        "np.random.seed(10121091)\n",
        "X_data = np.random.randn(N, 2)\n",
        "\n",
        "def generate_labels(X_data, num_classes):\n",
        "    \"\"\"\n",
        "    Generate classification labels for any number of classes using a general rule.\n",
        "    \"\"\"\n",
        "    N = X_data.shape[0]\n",
        "    y_data = np.zeros(N, dtype=int)  # Default all to class 0\n",
        "\n",
        "    if num_classes == 1:\n",
        "        return y_data\n",
        "\n",
        "    elif num_classes == 2:\n",
        "        return (X_data[:, 0] > 0).astype(int)\n",
        "\n",
        "    elif num_classes == 3:\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1\n",
        "        y_data[X_data[:, 1] <= 0] = 2\n",
        "\n",
        "    elif num_classes == 4:\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] <= 0)] = 2\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] <= 0)] = 3\n",
        "\n",
        "    elif num_classes == 5:\n",
        "        center_mask = (np.abs(X_data[:, 0]) < 0.5) & (np.abs(X_data[:, 1]) < 0.5)\n",
        "        y_data[center_mask] = 4\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] > 0)] = 0\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] > 0)] = 1\n",
        "        y_data[(X_data[:, 0] <= 0) & (X_data[:, 1] <= 0)] = 2\n",
        "        y_data[(X_data[:, 0] > 0) & (X_data[:, 1] <= 0)] = 3\n",
        "\n",
        "    else:\n",
        "        angles = np.arctan2(X_data[:, 1], X_data[:, 0])\n",
        "        angles[angles < 0] += 2 * np.pi\n",
        "\n",
        "        sector_size = 2 * np.pi / num_classes\n",
        "        y_data = (angles // sector_size).astype(int)\n",
        "    return y_data\n",
        "\n",
        "y_data = generate_labels(X_data, num_classes)\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the MLPClassifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=(20, 10), activation='relu', solver='adam',\n",
        "                    max_iter=1000, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv5UMP5AaCi8",
        "outputId": "216ce38a-7fa6-4544-ad6e-1f9935f340de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Multiclass MLP Reviews Using NumPy**"
      ],
      "metadata": {
        "id": "KImhhHtsfyTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Seed for reproducibility\n",
        "random.seed(10121091)\n",
        "np.random.seed(10121091)\n",
        "\n",
        "# Sample words for generating synthetic reviews\n",
        "positive_words = ['excellent', 'great', 'lovely', 'fantastic', 'wonderful']\n",
        "negative_words = ['terrible', 'poor', 'disappointing', 'bad', 'awful']\n",
        "neutral_words = ['okay', 'decent', 'average', 'fair', 'satisfactory']\n",
        "service_words = ['service', 'staff', 'waiter', 'manager', 'support']\n",
        "price_words = ['price', 'cost', 'charge', 'fee', 'expense']\n",
        "quality_words = ['quality', 'taste', 'flavor', 'preparation', 'ingredient']\n",
        "ambiance_words = ['ambiance', 'environment', 'atmosphere', 'setting', 'surroundings']\n",
        "\n",
        "# Generate synthetic reviews\n",
        "reviews = []\n",
        "labels = []\n",
        "N = 1000  # Number of reviews\n",
        "for _ in range(N):\n",
        "    review = []\n",
        "    label = set()\n",
        "\n",
        "    # Randomly choose sentiment\n",
        "    sentiment = random.choice(['positive', 'negative', 'neutral'])\n",
        "    if sentiment == 'positive':\n",
        "        review.append(random.choice(positive_words))\n",
        "        label.add(\"Positive\")\n",
        "    elif sentiment == 'negative':\n",
        "        review.append(random.choice(negative_words))\n",
        "        label.add(\"Negative\")\n",
        "    else:\n",
        "        review.append(random.choice(neutral_words))\n",
        "        label.add(\"Neutral\")\n",
        "\n",
        "    # Randomly add some topics\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(service_words))\n",
        "        label.add(\"Service\")\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(price_words))\n",
        "        label.add(\"Price\")\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(quality_words))\n",
        "        label.add(\"Quality\")\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(ambiance_words))\n",
        "        label.add(\"Ambiance\")\n",
        "\n",
        "    reviews.append(\" \".join(review))\n",
        "    labels.append(label)\n",
        "\n",
        "# Create a vocabulary and label mapping\n",
        "vocab = set()\n",
        "for review in reviews:\n",
        "    for word in review.split():\n",
        "        vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "label_set = set()\n",
        "for label in labels:\n",
        "    label_set.update(label)\n",
        "label_set = list(label_set)\n",
        "\n",
        "# Convert reviews to numerical features (bag of words)\n",
        "X = np.zeros((len(reviews), len(vocab)))\n",
        "for i, review in enumerate(reviews):\n",
        "    for word in review.split():\n",
        "        if word in vocab:\n",
        "            X[i, vocab.index(word)] += 1\n",
        "\n",
        "# Convert labels to binary matrix\n",
        "y = np.zeros((len(labels), len(label_set)))\n",
        "for i, label in enumerate(labels):\n",
        "    for l in label:\n",
        "        y[i, label_set.index(l)] = 1\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
        "    np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(X.shape[0] * (1 - test_size))\n",
        "    X_train, X_test = X[indices[:split]], X[indices[split:]]\n",
        "    y_train, y_test = y[indices[:split]], y[indices[split:]]\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss\n",
        "\n",
        "# Multi-layer perceptron (MLP) class with customizable hidden layers\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases for each layer\n",
        "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        # Forward pass through each layer\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            if i < len(self.weights) - 1:\n",
        "                a = relu(z)  # ReLU for hidden layers\n",
        "            else:\n",
        "                a = softmax(z)  # Softmax for output layer\n",
        "            self.activations.append(a)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        m = X.shape[0]\n",
        "        gradients = []\n",
        "\n",
        "        # Output layer error\n",
        "        dz = self.activations[-1] - y\n",
        "        gradients.append(dz)\n",
        "\n",
        "        # Backpropagate through hidden layers\n",
        "        for i in range(len(self.weights) - 1, 0, -1):\n",
        "            dz = np.dot(gradients[-1], self.weights[i].T) * relu_derivative(self.z_values[i - 1])\n",
        "            gradients.append(dz)\n",
        "\n",
        "        gradients.reverse()\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            dW = np.dot(self.activations[i].T, gradients[i]) / m\n",
        "            db = np.sum(gradients[i], axis=0, keepdims=True) / m\n",
        "            self.weights[i] -= learning_rate * dW\n",
        "            self.biases[i] -= learning_rate * db\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_entropy_loss(y, y_pred)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Define the architecture of the neural network\n",
        "input_size = X_train.shape[1]\n",
        "hidden_layers = [20, 20]  # Two hidden layers with 20 neurons each\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "# Initialize and train the MLP\n",
        "mlp = MLP(input_size, hidden_layers, output_size)\n",
        "mlp.train(X_train, y_train, learning_rate=0.01, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = mlp.predict(X_test)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcSvtDypaT1N",
        "outputId": "404cf2a2-96f3-4bfa-b871-5da05d58d23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.9459077111545002\n",
            "Epoch 100, Loss: 1.9361894204703056\n",
            "Epoch 200, Loss: 1.9325955156493018\n",
            "Epoch 300, Loss: 1.9331254184296671\n",
            "Epoch 400, Loss: 1.9368903944342089\n",
            "Epoch 500, Loss: 1.945029151312743\n",
            "Epoch 600, Loss: 1.9654954079151006\n",
            "Epoch 700, Loss: 2.029323670911669\n",
            "Epoch 800, Loss: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2cda16ab238b>:105: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
            "<ipython-input-9-2cda16ab238b>:99: RuntimeWarning: invalid value encountered in subtract\n",
            "  exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 900, Loss: nan\n",
            "Accuracy: 0.5433333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Multiclass MLP Reviews Using SKLearn**"
      ],
      "metadata": {
        "id": "9-QU3dwFgJzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Seed for reproducibility\n",
        "random.seed(10121091)\n",
        "\n",
        "# Sample words for generating synthetic reviews\n",
        "positive_words = ['excellent', 'great', 'lovely', 'fantastic', 'wonderful']\n",
        "negative_words = ['terrible', 'poor', 'disappointing', 'bad', 'awful']\n",
        "neutral_words = ['okay', 'decent', 'average', 'fair', 'satisfactory']\n",
        "service_words = ['service', 'staff', 'waiter', 'manager', 'support']\n",
        "price_words = ['price', 'cost', 'charge', 'fee', 'expense']\n",
        "quality_words = ['quality', 'taste', 'flavor', 'preparation', 'ingredient']\n",
        "ambiance_words = ['ambiance', 'environment', 'atmosphere', 'setting', 'surroundings']\n",
        "\n",
        "# Generate synthetic reviews\n",
        "reviews = []\n",
        "labels = []\n",
        "N = 1000\n",
        "for _ in range(N):  # Generate 1000 reviews\n",
        "    review = []\n",
        "    label = set()\n",
        "\n",
        "    # Randomly choose sentiment\n",
        "    sentiment = random.choice(['positive', 'negative', 'neutral'])\n",
        "    if sentiment == 'positive':\n",
        "        review.append(random.choice(positive_words))\n",
        "        label.add(\"Positive\")\n",
        "    elif sentiment == 'negative':\n",
        "        review.append(random.choice(negative_words))\n",
        "        label.add(\"Negative\")\n",
        "    else:\n",
        "        review.append(random.choice(neutral_words))\n",
        "        label.add(\"Neutral\")\n",
        "\n",
        "    # Randomly add some topics\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(service_words))\n",
        "        label.add(\"Service\")\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(price_words))\n",
        "        label.add(\"Price\")\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(quality_words))\n",
        "        label.add(\"Quality\")\n",
        "    if random.random() < 0.5:\n",
        "        review.append(random.choice(ambiance_words))\n",
        "        label.add(\"Ambiance\")\n",
        "\n",
        "    reviews.append(\" \".join(review))\n",
        "    labels.append(label)\n",
        "\n",
        "# Preprocessing\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(reviews).toarray()\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(labels)\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = OneVsRestClassifier(SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, verbose=1000))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QtC_srJaq-k",
        "outputId": "823a5c20-c686-4841-ea53-f80ff7bc2020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 43.73, NNZs: 35, Bias: -4.272408, T: 700, Avg. loss: 0.367791\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 35.31, NNZs: 35, Bias: -3.687017, T: 1400, Avg. loss: 0.013813\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.69, NNZs: 35, Bias: -3.390818, T: 2100, Avg. loss: 0.021517\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 31.87, NNZs: 35, Bias: -3.186692, T: 2800, Avg. loss: 0.025615\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 31.47, NNZs: 35, Bias: -3.129884, T: 3500, Avg. loss: 0.026651\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 31.32, NNZs: 35, Bias: -2.666942, T: 4200, Avg. loss: 0.027233\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 31.18, NNZs: 35, Bias: -3.043807, T: 4900, Avg. loss: 0.027633\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34.64, NNZs: 35, Bias: -1.995620, T: 700, Avg. loss: 0.143479\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27.80, NNZs: 35, Bias: -2.395465, T: 1400, Avg. loss: 0.007889\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 25.69, NNZs: 35, Bias: -2.418213, T: 2100, Avg. loss: 0.011976\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25.04, NNZs: 35, Bias: -2.011511, T: 2800, Avg. loss: 0.014114\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 24.77, NNZs: 35, Bias: -1.906368, T: 3500, Avg. loss: 0.014733\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 24.66, NNZs: 35, Bias: -1.746059, T: 4200, Avg. loss: 0.015194\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 24.59, NNZs: 35, Bias: -2.047912, T: 4900, Avg. loss: 0.015364\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 31.66, NNZs: 35, Bias: -1.860257, T: 700, Avg. loss: 0.155448\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 26.75, NNZs: 35, Bias: -1.512543, T: 1400, Avg. loss: 0.010087\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 25.32, NNZs: 35, Bias: -1.381237, T: 2100, Avg. loss: 0.013103\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 24.82, NNZs: 35, Bias: -1.592889, T: 2800, Avg. loss: 0.014355\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 24.60, NNZs: 35, Bias: -1.598055, T: 3500, Avg. loss: 0.014754\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 24.50, NNZs: 35, Bias: -1.811620, T: 4200, Avg. loss: 0.015139\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 24.48, NNZs: 35, Bias: -1.514235, T: 4900, Avg. loss: 0.015255\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 32.29, NNZs: 35, Bias: -2.375368, T: 700, Avg. loss: 0.192787\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27.14, NNZs: 35, Bias: -2.076414, T: 1400, Avg. loss: 0.009921\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 25.65, NNZs: 35, Bias: -2.237477, T: 2100, Avg. loss: 0.013335\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 25.19, NNZs: 35, Bias: -2.063279, T: 2800, Avg. loss: 0.014837\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 25.02, NNZs: 35, Bias: -1.836453, T: 3500, Avg. loss: 0.015375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 24.91, NNZs: 35, Bias: -2.021769, T: 4200, Avg. loss: 0.015544\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 24.86, NNZs: 35, Bias: -2.014908, T: 4900, Avg. loss: 0.015681\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41.38, NNZs: 35, Bias: -2.579129, T: 700, Avg. loss: 0.349105\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 34.43, NNZs: 35, Bias: -2.678229, T: 1400, Avg. loss: 0.016441\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.38, NNZs: 35, Bias: -3.061258, T: 2100, Avg. loss: 0.023390\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 31.75, NNZs: 35, Bias: -3.075319, T: 2800, Avg. loss: 0.026702\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 31.45, NNZs: 35, Bias: -3.040352, T: 3500, Avg. loss: 0.027402\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 31.35, NNZs: 35, Bias: -2.920803, T: 4200, Avg. loss: 0.028148\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 31.26, NNZs: 35, Bias: -2.811304, T: 4900, Avg. loss: 0.027957\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.25, NNZs: 35, Bias: -3.120801, T: 700, Avg. loss: 0.390781\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 35.72, NNZs: 35, Bias: -2.624538, T: 1400, Avg. loss: 0.014335\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 33.09, NNZs: 35, Bias: -2.711016, T: 2100, Avg. loss: 0.022506\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 32.17, NNZs: 35, Bias: -2.630071, T: 2800, Avg. loss: 0.026054\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 31.77, NNZs: 35, Bias: -2.740171, T: 3500, Avg. loss: 0.027507\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 31.56, NNZs: 35, Bias: -2.914379, T: 4200, Avg. loss: 0.028131\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 31.48, NNZs: 35, Bias: -2.483151, T: 4900, Avg. loss: 0.028467\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "-- Epoch 1\n",
            "Norm: 45.10, NNZs: 35, Bias: -4.239191, T: 700, Avg. loss: 0.381074\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 35.94, NNZs: 35, Bias: -3.443089, T: 1400, Avg. loss: 0.012909\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.99, NNZs: 35, Bias: -3.375700, T: 2100, Avg. loss: 0.021103\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 31.96, NNZs: 35, Bias: -3.183604, T: 2800, Avg. loss: 0.025035\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 31.58, NNZs: 35, Bias: -2.981250, T: 3500, Avg. loss: 0.027041\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 31.34, NNZs: 35, Bias: -3.400078, T: 4200, Avg. loss: 0.027519\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 31.26, NNZs: 35, Bias: -3.151113, T: 4900, Avg. loss: 0.028022\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}